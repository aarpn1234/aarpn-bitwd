name: Backup Production D1 Database to GCP Storage

on:
  schedule:
    - cron: "0 5 * * *"  # Daily at 05:00 UTC
  workflow_dispatch:

env:
  BACKUP_RETENTION_DAYS: 30
  BACKUP_PREFIX: "vault1_prod"

jobs:
  backup-gcp:
    name: Backup Production Database to GCP
    runs-on: ubuntu-latest

    steps:
      - name: Get current date
        id: date
        run: echo "date=$(date +'%Y-%m-%d_%H-%M-%S')" >> $GITHUB_OUTPUT

      - name: Install dependencies
        run: |
          npm install -g wrangler
          sudo apt-get update
          sudo apt-get install -y jq google-cloud-sdk

      - name: Get D1 Database Name
        id: db_name
        env:
          CLOUDFLARE_API_TOKEN: ${{ secrets.CLOUDFLARE_API_TOKEN }}
          CLOUDFLARE_ACCOUNT_ID: ${{ secrets.CLOUDFLARE_ACCOUNT_ID }}
          D1_DATABASE_ID: ${{ secrets.D1_DATABASE_ID }}
        run: |
          if [ -z "$D1_DATABASE_ID" ]; then
            echo "âŒ Error: D1_DATABASE_ID secret is missing"
            exit 1
          fi
          
          DB_NAME=$(npx wrangler d1 list --json | jq -r ".[] | select(.uuid == \"$D1_DATABASE_ID\") | .name")
          if [ -z "$DB_NAME" ] || [ "$DB_NAME" = "null" ]; then
            echo "âŒ Error: Could not find database with ID: $D1_DATABASE_ID"
            exit 1
          fi
          echo "Database found: $DB_NAME"
          echo "db_name=$DB_NAME" >> $GITHUB_OUTPUT

      - name: Export D1 Database
        env:
          CLOUDFLARE_API_TOKEN: ${{ secrets.CLOUDFLARE_API_TOKEN }}
          CLOUDFLARE_ACCOUNT_ID: ${{ secrets.CLOUDFLARE_ACCOUNT_ID }}
        run: |
          echo "Exporting D1 database: ${{ steps.db_name.outputs.db_name }}"
          npx wrangler d1 export "${{ steps.db_name.outputs.db_name }}" \
            --remote \
            --output=backup.sql
          
          if [ ! -f "backup.sql" ]; then
            echo "âŒ Error: Database export failed - backup.sql not created"
            exit 1
          fi
          
          gzip backup.sql
          echo "âœ… Backup exported and compressed: backup.sql.gz"
          ls -lh backup.sql.gz

      - name: Encrypt backup (if encryption key provided)
        env:
          BACKUP_ENCRYPTION_KEY: ${{ secrets.BACKUP_ENCRYPTION_KEY }}
        run: |
          if [ -n "$BACKUP_ENCRYPTION_KEY" ] && [ "$BACKUP_ENCRYPTION_KEY" != "" ]; then
            echo "ðŸ”’ Encrypting backup..."
            openssl enc -aes-256-cbc -salt -pbkdf2 -iter 100000 \
              -in backup.sql.gz \
              -out "${{ env.BACKUP_PREFIX }}_${{ steps.date.outputs.date }}.sql.gz.enc" \
              -pass pass:"$BACKUP_ENCRYPTION_KEY"
            
            if [ $? -ne 0 ]; then
              echo "âŒ Error: Backup encryption failed"
              exit 1
            fi
            
            rm backup.sql.gz
            ENCRYPTED_FILE="${{ env.BACKUP_PREFIX }}_${{ steps.date.outputs.date }}.sql.gz.enc"
            echo "encrypted_file=$ENCRYPTED_FILE" >> $GITHUB_OUTPUT
            echo "âœ… Encrypted backup: $ENCRYPTED_FILE"
          else
            mv backup.sql.gz "${{ env.BACKUP_PREFIX }}_${{ steps.date.outputs.date }}.sql.gz"
            UNENCRYPTED_FILE="${{ env.BACKUP_PREFIX }}_${{ steps.date.outputs.date }}.sql.gz"
            echo "encrypted_file=" >> $GITHUB_OUTPUT
            echo "unencrypted_file=$UNENCRYPTED_FILE" >> $GITHUB_OUTPUT
            echo "âœ… Unencrypted backup: $UNENCRYPTED_FILE"
          fi

      - name: Authenticate to GCP
        uses: google-github-actions/auth@v2
        with:
          credentials_json: ${{ secrets.GCP_SERVICE_ACCOUNT }}

      - name: Set up Cloud SDK
        uses: google-github-actions/setup-gcloud@v2
        with:
          project_id: ${{ secrets.GCP_PROJECT_ID }}

      - name: Upload backup to GCP Cloud Storage
        env:
          BACKUP_ENCRYPTION_KEY: ${{ secrets.BACKUP_ENCRYPTION_KEY }}
        run: |
          if [ -n "$BACKUP_ENCRYPTION_KEY" ] && [ "$BACKUP_ENCRYPTION_KEY" != "" ]; then
            BACKUP_FILE="${{ env.BACKUP_PREFIX }}_${{ steps.date.outputs.date }}.sql.gz.enc"
          else
            BACKUP_FILE="${{ env.BACKUP_PREFIX }}_${{ steps.date.outputs.date }}.sql.gz"
          fi

          if [ ! -f "$BACKUP_FILE" ]; then
            echo "âŒ Error: Backup file does not exist: $BACKUP_FILE"
            exit 1
          fi

          BUCKET_PATH="gs://${{ secrets.GCS_BUCKET }}/warden-worker/production/"
          UPLOAD_DEST="$BUCKET_PATH$BACKUP_FILE"
          
          gsutil cp "$BACKUP_FILE" "$UPLOAD_DEST"
          
          if [ $? -eq 0 ]; then
            echo "âœ… Successfully uploaded to: $UPLOAD_DEST"
            # Get file size for logging (without exposing sensitive content)
            FILE_SIZE=$(stat -c%s "$BACKUP_FILE")
            echo "Backup size: $((FILE_SIZE / 1024 / 1024)) MB"
          else
            echo "âŒ Error: Failed to upload backup to GCP"
            exit 1
          fi

      - name: Cleanup old GCP backups
        run: |
          if [ -z "${{ secrets.GCS_BUCKET }}" ]; then
            echo "âŒ Error: GCS_BUCKET secret is missing"
            exit 1
          fi
          
          CUTOFF_DATE=$(date -d "-${{ env.BACKUP_RETENTION_DAYS }} days" +%Y-%m-%d)
          echo "Looking for files older than $CUTOFF_DATE"
          
          # List all backup files in the bucket
          ALL_FILES=$(gsutil ls "gs://${{ secrets.GCS_BUCKET }}/warden-worker/production/${{ env.BACKUP_PREFIX }}_"* 2>/dev/null || true)
          
          if [ -z "$ALL_FILES" ]; then
            echo "No backup files found to clean up"
            exit 0
          fi
          
          # Process each file
          echo "$ALL_FILES" | while read -r file_url; do
            if [ -n "$file_url" ]; then
              # Extract filename from URL
              filename=$(basename "$file_url")
              
              # Extract date from filename (format: prefix_YYYY-MM-DD_HH-MM-SS.ext)
              file_date_part=$(echo "$filename" | grep -oE '[0-9]{4}-[0-9]{2}-[0-9]{2}' | head -c 10)
              
              if [ -n "$file_date_part" ]; then
                # Compare dates directly
                if [[ "$file_date_part" < "$CUTOFF_DATE" ]]; then
                  echo "Deleting old backup: $filename (date: $file_date_part)"
                  gsutil rm "$file_url"
                  
                  if [ $? -eq 0 ]; then
                    echo "âœ… Deleted: $filename"
                  else
                    echo "âŒ Failed to delete: $filename"
                  fi
                else
                  echo "Keeping recent backup: $filename (date: $file_date_part)"
                fi
              else
                echo "Skipping non-backup file: $filename"
              fi
            fi
          done
          
          echo "âœ… GCP cleanup completed"